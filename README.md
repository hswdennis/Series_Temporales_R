# Series Temporales en R

![banner_serie_temporal](img/banner.png)

## Introducci√≥n

Esta tarea personal sirve para la puesta en pr√°ctica del estudio de series temporales. 
He descargado un conjunto de datos de la p√°gina web de [Kaggle](https://www.kaggle.com/datasets/sudalairajkumar/daily-temperature-of-major-cities) que contiene los valores de temperatura media diaria de las principales ciudades del mundo. Dichas temperaturas son recogidas en Fahrenheit desde el 1 de enero de 1995 hasta el 13 de mayo de 2020. 

Las caracter√≠sticas m√°s importantes a tener en cuenta para nuestro estudio son las siguientes:

- **Region**: Nombre de la regi√≥n.
- **Country**: Pa√≠s.
- **State**: Nombre del estado (en caso de tenerlo).
- **City**: Nombre de la ciudad
- **Month**: N√∫mero del mes. 
- **Day**: N√∫mero del d√≠a.
- **Year**: A√±o al que pertenece la medida.
- **AvgTemperature**: Temperatura promedio de la entrada en fahrenheit.

El objetivo de este estudio es intentar predecir la tendencia de temperatura en los pr√≥ximos a√±os. Como el tama√±o de los datos es excesivamente grande, voy a filtrar y focalizar el estudio en una ciudad en concreto. 

Para esta tarea, recogeremos el primer valor de temperatura de cada mes de la ciudad de Madrid desde el a√±o 2000 hasta el 2014 incluido. En estos 14 a√±os de datos encontramos un total de 168 observaciones para estudiar. Para la predicci√≥n hay margen hasta el 2020, pero con intentar predecir las siguientes 12 observaciones tenemos suficientes (lo que equivale predecir al a√±o 2015 en su totalidad).

## Representaci√≥n gr√°fica, descomposici√≥n y an√°lisis de los datos

Al seleccionar el primer valor de cada mes, se nos queda una un conjunto de periodo 12. Si visualizamos nuestros datos observamos que tienen una media constante y una varianza
acotada. 

![Valores_temperatura](img/1_Valores_temperatura.png)

Con el gr√°fico anterior, somos capaces de confirmar que los datos se ajustan a un esquema aditivo. Con la descomposici√≥n estacional concluimos que febrero es el mes m√°s fr√≠o (-17.20) y agosto el m√°s caluroso (+22.96).

Para el an√°lisis de las serioes temporales aditivas, es necesaria desglosar su informaci√≥n en tres componentes principales: tendencia, estacionalidad y residuo. Esto nos facilita el an√°lisis individual de cada componente y as√≠ comprender mejor el comportamiento de los datos a trav√©s del an√°lisis de patrones o ifentificaci√≥n de anomal√≠as.

![descomposicion_estacional](img/3_Descomposicion_estacional_2.png)

En la imagen anterior se encuentran representadas las principales caracter√≠siticas que construyen una serie temporal:

- **Data**: Representa la serie temporal completa. Es la suma de los tres componentes de tendencia, estacionalidad y residuo. Podemos ver oscilaciones regulares (que indican estacionalidad), y ligeros cambios (indican tendencia).
- **Trend**: Refleja la tendencia de los cambios durante el periodo. En este caso podemos apreciar un aumento gradual entre el a√±o 2000 y 2005, seguido de una ca√≠da y una recuperaci√≥n hasta 2015.
- **Seasonal**: La componente estacional son variaciones peri√≥dicas c√≠clicas que se repiten de manera regular. Nuestra periocidad es anual y casi id√©ntica por cada ciclo.
- **Remainder**: Con el residuo reflejamos aquello que no puede explicarse ni por la tendencia o la estiacionalidad. En nuestro caso, no podemoso observar un patr√≥n claro, lo que sugiere variabilidad residual.

Agrupar la informaci√≥n de cada mes a lo largo del a√±o tambi√©n puede ayudar a entender nuestro caso de uso. 

![descomposicion_estacional_2](img/5_Descomposicion_estacional_4.png)

Observando el par de im√°genes anteriores concluimos que la serie tiene un comportamiento estacional que se repite cada a√±o, donde agosto despunta en contener los valores m√°s altos. Adem√°s, observamos una min√∫scula tendencia que aumenta con los a√±os. 

## Modelados de suavizado exponencial

Los modelos de suavizado exponencial nos sirven para predecir valores futuros bas√°ndose en un promedio ponderado de valores pasdados. Estos modelos son ampliamente usados debido a su simplicidad, flexibilidad y capacidad para adaptarse a datos con componentes de tendencia y estacionalidad. Tres de los modelos m√°s utilizados son:

### M√©todo simple

Usados para series que no representan tendencia ni estacionalidad. Usamos la funcion *ses()* y predecimos los siguientes 12 meses.

```
temperatura_ses=ses(temperatura_TR, h=12)
summary(temperatura_ses)

autoplot(temperatura_ses) + autolayer(fitted(temperatura_ses), series="Fitted") + ylab("Temperatura") + xlab("A√±o")
```

![descomposicion_estacional_2](img/6_Suavizado_simple.png)

### M√©todo Alisado Doble de Holt

Usados para series con tendencia y sin estacionalidad. Utilizaremos la funcion *holt()* y predecimos los siguientes 12 meses.

```
temperatura_sh <- holt(temperatura_TR, h=12)    
summary(temperatura_sh)

autoplot(temperatura_sh) + autolayer(fitted(temperatura_sh), series="Fitted") + ylab("Temperatura") + xlab("A√±o")
```

![Suavizado holt](img/7_Suavizado_Holt.png)

### M√©todo alisado Holt-Winters

Este m√©todo incluye las mejoras para trabajar con series estacionales y con tendencia. La funcion *hw()* nos permite introducir que su naturaleza sea aditiva. Predecimos los siguientes 12 meses.

```
fit1 <- hw(temperatura_TR, h=12, seasonal="additive")
summary(fit1)

autoplot(fit1) + autolayer(fitted(fit1), series="Fitted") + ylab("temperatura)") + xlab("A√±o")
```

![Suavizado Holt-Winters](img/8_Suavizado_Holt_Winters.png)

Concluimos que el m√©todo que mejor ajusta y predice nuestra serie es el m√©todo de alisado de Holt-Winters. Gracias a la funci√≥n  *summary()* del modelo ganador obtenemos los siguientes valores de Alpha, Beta y Gamma con el que construir la expresi√≥n del m√©todo ganador.

ùêøùë° = 1e-04 (ùë•ùë°/ùëÜùë°‚àíùë†) + (1 ‚àí 1e-04) (ùêøùë°‚àí1 + ùëèùë°‚àí1)

b_t=„Äñ1e-04 (L„Äó_t-L_(t-1 ))+ 1e-04 b_(t-1)

ùëÜt = 1e-04 (ùë•ùë°/ùêøùë° ) + ( 1 ‚àí 1e-04) ùëÜt-s 

x ÃÇùë°+1 = (ùêøùë° + ùëèùë° ) ùëÜùë°‚àíùë†+1

![Expresion algebraica](img/9_Expression.png)

## Funciones de autocorrelaci√≥n simple y espacial

Es el momento de extraer los valores de autocorrelaci√≥n simple (ACF) y espacial (PACF). Estas cantidades miden la relaci√≥n lineal entre las variables de la serie separadas por *k* posiciones. Para la variante espacial, las posiciones est√°n separadas por el tiempo. Ambas funciones nos ayudan a determinar los par√°metros de los modelos que vamos a estudiar.

```
ggAcf(temperatura_TR, lag=48)
```

![Correlacion ACE](img/10_Correlacion_ACF.png)

```
ggPacf(temperatura_TR,lag=48) 
```

![Correlacion PACE](img/10_Correlacion_PACF.png)

Recordamos que la variable temperatura_TR es la ventana de datos extra√≠dos para la parte de predicci√≥n, desde el a√±o 2000 hasta el 2014.

Se observa un comportamiento repetitivo de las autocorrelaciones cada 12 meses en la ACF, observando como la autocorrelaci√≥n m√°s fuerte es en los retardos m√∫ltiplos de 12. Como la serie presenta estacionalidad, por lo que tomamos una docena de diferenciaci√≥n.

```
ggAcf(diff(temperatura_TR), lag=48)
```
![Correlacion ACE 2](img/12_Correlacion_ACF_12.png)

```
ggPacf(diff(temperatura_TR), lag=48)
```
![Correlacion PACE 2](img/13_Correlacion_PACF_12.png)


Procedamos a ajustar el modelo adecuado. Para ello, elaboraremos diferentes modelos con el fin de compararlos y elegir un ganador a trav√©s de la funci√≥n *arima()*. 
En el determinamos los par√°metros del modelo siguiendo unos comportamientos en las gr√°ficas de ACF y PACF. En este caso, como nuestra serie tiene una estacionalidad 12, solo hay que observar los retardos m√∫ltiplos de 12. 

```
t_fitARIMA_1<-arima(temperatura_TR,order=c(0,1,0), seasonal=c(0,1,1))
t_fitARIMA_2<-arima(temperatura_TR,order=c(1,1,1), seasonal=c(0,1,1))
t_fitARIMA_3<-arima(temperatura_TR,order=c(0,1,0), seasonal=c(0,1,3))
t_fitARIMA_4<-arima(temperatura_TR,order=c(0,0,1), seasonal=c(1,1,0))
```

Con la funci√≥n *summary()* seremos capaces de recuperar los coeficientes sigma estimado y medidas de error de entrenamiento para poder compararlos todos. Sin embargo, la funci√≥n *coeftest()* nos comprueba la idoneidad del modelo autom√°ticamente. 

### Modelo 1

![Modelo 1 resultados](img/Modelo_1res.png)
![Modelo 1 imagen](img/Modelo_1pic.png)

### Modelo 2

![Modelo 2 resultados](img/Modelo_2res.png)
![Modelo 2 imagen](img/Modelo_2pic.png)

### Modelo 3

![Modelo 3 resultados](img/Modelo_3res.png)
![Modelo 3 imagen](img/Modelo_3pic.png)

### Modelo 4

![Modelo 4 resultados](img/Modelo_4res.png)
![Modelo 4 imagen](img/Modelo_4pic.png)

### Resultados

Antes de determinar un ganador deberemos comprobar los residuos en el que, si el modelo se aproxima satisfactoriamente a la serie observada, los residuos deben tender a comportarse como ruido blanco. Es por eso por lo que el residuo que m√°s uniforme parezca es el mejor. Viendo las siguientes ilustraciones podemos quedarnos entre dos modelos, el segundo y cuarto. 

Despu√©s de un an√°lisis, selecciono como ganador el modelo 4. El factor diferenciador para seleccionar como ganador un modelo, es que tiene menos coeficientes (coeftest).

La expresi√≥n algebraica del modelo ganador es la siguiente:

![Expresion_algebraica_2](img/15_Expression.png)

## Predicciones e intervalos de confianza

Es el momento de calcular la predicci√≥n y los intervalos de confianza para el modelo ganador (modelo 4). La funci√≥n *forecast()* recibe el modelo deseado y el numero de unidades a predecir. En este caso, veo razonable intentar predecir **24 meses**. 

```
# Calculo de las predicciones y los intervalos de confianza para el modelo mas adecuado (t_fitARIMA_4)
forecast(t_fitARIMA_4,h=24)
summary(forecast(t_fitARIMA_4,h=24))

# Representar las predicciones obtenidas
autoplot(forecast(t_fitARIMA_4),h=24)
```

![Prediccion modelo ARIMA](img/16_Prediccion_modelo_ARIMA.png)
![Prediccion modelo Holt-Winters](img/17_Prediccion_modelo_HoltWinters.png)

Como mencion√© al principio, nuestros datos iban dese el a√±o 1995 hasta el 2020, pero trabajamos del 2000 al 2015, tenemos la suerte de poder comparar la predicci√≥n con los datos reales (las im√°genes est√°n alineadas). 

![Datos reales de temperatura](img/18_Datos_reales.png.png)


Observando las tres im√°genes, se ve a primera vista que la estimaci√≥n realizada con el modelo Holt-Winters parece la m√°s ajustada. En los datos reales, se da que el a√±o 2016 se generan picos de temperatura. Comparando y usando como referencia el a√±o 2013, el 2016 sobre pasa muy levemente el pico de temperatura. Los intervalos de confianza en el modelo ARIMA son m√°s ‚Äúgrandes‚Äù, lo que indica mayor margen de error, por el otro lado en el modelo Holt-Winters, el borde del intervalo de confianza del 95% coincide con los datos reales. Es por eso por lo que concluyo que el modelo Holt-Winters est√° mejor ajustado, sin embargo, el de ARIMA predice mejor ya que tiene en cuenta (y por eso dejo el margen) una mayor variaci√≥n.

